{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a0544-ba44-4649-87a2-4448aa7bafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (do not containerize this cell)\n",
    "param_data_filename <- \"Template_MBO_Example_raw.xlsx\"\n",
    "param_metadata_sheet <- \"METADATA\"\n",
    "param_data_sheet <- \"BIRDS\"\n",
    "param_user_name <- \"See URL: beta.naavre.net/jupyter/user/[param_user_name]/lab\"\n",
    "param_use_dummy_data <- 1\n",
    "param_years <- 7\n",
    "param_latitude_north <- 90.0000\n",
    "param_latitude_south <- 25.0000\n",
    "param_longitude_east <- 70.0000\n",
    "param_longitude_west <- 0.0000\n",
    "param_upper_limit_max_depth <- 0\n",
    "param_lower_limit_max_depth <- 50\n",
    "param_upper_limit_min_depth <- 0\n",
    "param_lower_limit_min_depth <- 50\n",
    "param_first_month <- 1\n",
    "param_last_month <- 3\n",
    "conf_temporary_data_directory <- \"/tmp/data\"\n",
    "conf_virtual_lab_biotisan_euromarec <- \"vl-biotisan-euromarec\"\n",
    "conf_minio_endpoint <- \"scruffy.lab.uvalight.net:9000\"\n",
    "conf_minio_region <- \"nl-uvalight\"\n",
    "conf_minio_public_bucket <- \"naa-vre-public\"\n",
    "conf_minio_user_bucket <- \"naa-vre-user-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a2837-0613-49ac-9092-2cf4f73dac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secrets (do not containerize this cell)\n",
    "library(\"SecretsProvider\")\n",
    "\n",
    "secretsProvider <- SecretsProvider()\n",
    "secret_minio_access_key = \"\"\n",
    "secret_minio_access_key = secretsProvider$get_secret(\"secret_minio_access_key\")\n",
    "secret_minio_secret_key = \"\"\n",
    "secret_minio_secret_key = secretsProvider$get_secret(\"secret_minio_secret_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7a47a-31d5-4c61-9bae-183b1d07d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinIO data retriever\n",
    "library(\"readxl\")\n",
    "library(\"aws.s3\")\n",
    "\n",
    "Sys.setenv(\"AWS_S3_ENDPOINT\" = conf_minio_endpoint,\n",
    "           \"AWS_DEFAULT_REGION\" = conf_minio_region,\n",
    "           \"AWS_ACCESS_KEY_ID\" = secret_minio_access_key,\n",
    "           \"AWS_SECRET_ACCESS_KEY\" = secret_minio_secret_key)\n",
    "\n",
    "# Download file from bucket S3\n",
    "if (param_use_dummy_data) {\n",
    "        file_path <- paste(conf_virtual_lab_biotisan_euromarec, param_data_filename, sep=\"/\")\n",
    "        print(sprintf(\"Using dummy data for testing purposes. Set param_use_dummy_data to 0 to use your own data. Downloading data from %s/%s\", conf_minio_public_bucket, file_path))\n",
    "        aws.s3::save_object(bucket=conf_minio_public_bucket, object=file_path, file=param_data_filename)\n",
    "    } else {\n",
    "        file_path <- paste(param_user_name, param_data_filename, sep=\"/\")\n",
    "        print(sprintf(\"Downloading data from %s / %s\", conf_minio_user_bucket, file_path))\n",
    "        aws.s3::save_object(bucket=conf_minio_user_bucket, object=file_path, file=param_data_filename)\n",
    "}\n",
    "\n",
    "# Load data & metadata\n",
    "metadata <- read_excel(param_data_filename, sheet = param_metadata_sheet) #Load metadata sheet\n",
    "data <- read_excel(param_data_filename, sheet = param_data_sheet) #Load data sheet\n",
    "\n",
    "# Ensure the temporary data storage directory exists\n",
    "dir.create(conf_temporary_data_directory, showWarnings = FALSE)\n",
    "\n",
    "# Write (meta)data to files\n",
    "metadata_as_csv_filename <- \"metadata.csv\"\n",
    "data_as_csv_filename <- \"data.csv\"\n",
    "metadata_from_excel_path <- paste(conf_temporary_data_directory, metadata_as_csv_filename, sep=\"/\")\n",
    "data_from_excel_path <- paste(conf_temporary_data_directory, data_as_csv_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing metadata in: %s, and data in %s\", metadata_from_excel_path, data_from_excel_path))\n",
    "write.csv(metadata, file = metadata_from_excel_path)\n",
    "write.csv(data, file =  data_from_excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c595e0-33ba-4aae-890f-ad44c397429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Species occurence data cleaner\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "\n",
    "validate_dataframe_has_data <- function(dataframe, dataframe_name) {\n",
    "    if (nrow(dataframe) == 0) {\n",
    "    stop(paste0(dataframe_name, \" has no rows (0 rows). Halting execution.\"))\n",
    "  } else {\n",
    "    sprintf(\"%s has %i rows.\", dataframe_name, nrow(dataframe))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Report on parameters set\n",
    "print(sprintf(\"Filtering sites with %i or more years of data.\", param_years))\n",
    "print(sprintf(\"Filtering for data within the coordinates %f south, %f north, %f east and %f west.\", param_latitude_south, param_latitude_north, param_longitude_east, param_longitude_west))\n",
    "print(sprintf(\"Filtering for data within upper_limit_max_depth %i, lower_limit_max_depth %i, upper_limit_min_depth %i, lower_limit_min_depth %i.\", param_upper_limit_max_depth, param_lower_limit_max_depth, param_upper_limit_min_depth, param_lower_limit_min_depth))\n",
    "print(sprintf(\"Filtering for data between the month %i and month %i.\", param_first_month, param_last_month))\n",
    "\n",
    "# Assign dummy variables to prevent false Input/Output detection by the NaaVRE cell analyzer\n",
    "datecollected = \"\"\n",
    "siteid = \"\"\n",
    "decimallatitude = \"\"\n",
    "decimallongitude = \"\"\n",
    "\n",
    "# Read (meta)data from files\n",
    "md <- read.csv(paste(conf_temporary_data_directory, metadata_as_csv_filename, sep=\"/\"), sep=\",\")\n",
    "data <- read.csv(paste(conf_temporary_data_directory, data_as_csv_filename, sep=\"/\"), sep=\",\")\n",
    "validate_dataframe_has_data(data, \"data read from csv\")\n",
    "validate_dataframe_has_data(md, \"metadata read from csv\")\n",
    "\n",
    "###### Years filter ######\n",
    "###### The \"number of sampled years\" could be changed by the user (default = 7) ######\n",
    "# Create a table with sites with more than 7 sampling years\n",
    "sites <- data %>% \n",
    "  group_by(siteid) %>%\n",
    "  summarise(nyear = n_distinct(substr(datecollected, 1, 4))) %>%\n",
    "  filter(nyear > param_years)\n",
    "\n",
    "md <- merge(md,sites, by = \"siteid\")\n",
    "validate_dataframe_has_data(md, \"metadata merged by siteid\")\n",
    "validate_dataframe_has_data(data, \"data after first years filter\")\n",
    "\n",
    "###### Coordinates filter ######\n",
    "###### The \"geographical boundaries\" could be changed by the user (default = latitude (25:90), longitude (-45:70)). The default values correspond to European continental waters. ######\n",
    "# Keep sites within the study area [our boundaries are latitude (25:90), longitude (-45:70)]\n",
    "metadata_coordinates <- md %>% select(siteid, decimallatitude, decimallongitude)\n",
    "print(metadata_coordinates)\n",
    "\n",
    "md <- dplyr::filter(md, decimallatitude >= param_latitude_south, decimallatitude <= param_latitude_north, \n",
    "                 decimallongitude >= param_longitude_west, decimallongitude <= param_longitude_east)\n",
    "print(paste0(\"Number of sites found within the specified geolocation: \", nrow(md)))\n",
    "\n",
    "data <- data %>% # Keep data from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "validate_dataframe_has_data(data, \"data filtered by coordinates\")\n",
    "\n",
    "###### Depth filter ######\n",
    "###### The \"depth\" could be changed by the user (default = 0 to 50 meters). It should be given in absolute value. ######\n",
    "#(in this case is not necessary, but for other taxonomic groups is possible that the sample were taken at different depths. The code keeps the NAs in case that information is not known)\n",
    "data <- data %>% filter((maximumdepthinmeters >= param_upper_limit_max_depth & maximumdepthinmeters <= param_lower_limit_max_depth) %>% tidyr::replace_na(TRUE))\n",
    "data <- data %>% filter((minimumdepthinmeters >= param_upper_limit_min_depth & minimumdepthinmeters <= param_lower_limit_min_depth) %>% tidyr::replace_na(TRUE))\n",
    "validate_dataframe_has_data(data, \"data filtered by depth\")\n",
    "\n",
    "###### Season filter ######\n",
    "###### The \"season\" could be changed by the user (default = 1:3). The default values correspond to winter.\n",
    "# The period does not need to be three months, can be 1:12 for the whole year. It cannot choose months from different years (for example, November to January) ######\n",
    "# In this case, most of the sampling campaigns were conducted in winter\n",
    "# One was conducted in summer and should be removed since the sampling season is not consistent\n",
    "data$month <- as.numeric(format(as.Date(data$datecollected), \"%m\")) # Create a column with the sampling month\n",
    "\n",
    "season <- c(param_first_month:param_last_month)\n",
    "data <- data %>%\n",
    "  filter(month %in% season) #Remove those samples in non-consistent seasons (in this case keeps the months 1, 2 and 3, this is January, February and March)\n",
    "validate_dataframe_has_data(data, \"data filtered by season\")\n",
    "\n",
    "# Note that some time series can have more than one sampling campaign per year and even per season (not in this case)\n",
    "# For our analysis, we are only keeping one sampling campaign per year\n",
    "\n",
    "###### Years filter ######\n",
    "###### Note that this step is repeated ######\n",
    "# Update the table with sites with more than the number of sampled years\n",
    "# After removing inconsistent sampling campaigns, some time series may become shorter than 8 years\n",
    "sites <- data %>% \n",
    "  group_by(siteid) %>%\n",
    "  summarise(nyear = n_distinct(substr(datecollected, 1, 4))) %>%\n",
    "  filter(nyear > param_years)\n",
    "\n",
    "data <- data %>% # Keep data from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "md <- md %>% # Keep metadata from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "validate_dataframe_has_data(data, \"data filtered by years\")\n",
    "\n",
    "md_final <- md[,c(1:8)]\n",
    "data_final <- data[,c(1:15)]\n",
    "\n",
    "# Write data to files\n",
    "cleaned_metadata_filename <- \"metadata_Example.csv\"\n",
    "cleaned_data_filename <- \"data_Example.csv\"\n",
    "cleaned_metadata_path <- paste(conf_temporary_data_directory, cleaned_metadata_filename, sep=\"/\")\n",
    "cleaned_data_path <- paste(conf_temporary_data_directory, cleaned_data_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing metadata in: %s, and data in %s\", cleaned_metadata_path, cleaned_data_path))\n",
    "write.csv(md_final, file = cleaned_metadata_path)\n",
    "write.csv(data_final, file =  cleaned_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e93b1c-73ab-44cd-9a54-4674d75a6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend analyzer\n",
    "library(vegan)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(nlme)\n",
    "\n",
    "# Assign dummy variables to prevent false Input/Output detection by the NaaVRE cell analyzer\n",
    "datecollected = \"\"\n",
    "parameter_value = \"\"\n",
    "\n",
    "# Load cleaned data & metadata\n",
    "cleaned_metadata_path <- paste(conf_temporary_data_directory, cleaned_metadata_filename, sep=\"/\")\n",
    "cleaned_data_path <- paste(conf_temporary_data_directory, cleaned_data_filename, sep=\"/\")\n",
    "print(sprintf(\"Retrieving metadata from: %s, and data from %s\", cleaned_metadata_path, cleaned_data_path))\n",
    "md <- read.csv(cleaned_metadata_path, sep=\",\")\n",
    "data <- read.csv(cleaned_data_path, sep=\",\")\n",
    "data$year <- as.numeric(format(as.Date(data$datecollected), \"%Y\"))\n",
    "colnames(data)\n",
    "\n",
    "# Validate data exists\n",
    "if (nrow(data) == 0) {\n",
    "    stop(\"data has no rows (0 rows). Halting execution.\")\n",
    "  } else {\n",
    "    sprintf(\"The data has %i rows:\", nrow(data))\n",
    "    }\n",
    "\n",
    "# Calculate community metrics\n",
    "data.tax <- data %>%\n",
    "  group_by(siteid, year, datecollected) %>%\n",
    "  summarise(richness = n_distinct(taxaname[parameter_value > 0]), # Richness\n",
    "            parameter_value_tot = sum(parameter_value), # Abundance estimate\n",
    "            parameter = unique(parameter),\n",
    "            parameter_standardunit = unique(parameter_standardunit),\n",
    "            diversity = diversity(parameter_value, index=\"shannon\"), # Diversity\n",
    "            )\n",
    "\n",
    "# Temporal analysis. Example with Richness and these 2 time series\n",
    "results.richness <- data.frame(siteid = character(0), slope = numeric(0), p = numeric(0))\n",
    "\n",
    "for (i in names(table(data.tax$siteid))) {\n",
    "  x <- subset(data.tax, siteid == i)\n",
    "  # We used GLS models taking into account the temporal autocorrelation\n",
    "  gls_model <- gls(log10(richness+1) ~ year, data = x, correlation = corAR1(form = ~ 1 | year))\n",
    "  slope <- coef(gls_model)[2]\n",
    "  p <- summary(gls_model)$tTable[2, 4]\n",
    "\n",
    "  # Save results\n",
    "  results.richness <- rbind(results.richness, data.frame(siteid = i, slope = slope, p = p))\n",
    "}\n",
    "\n",
    "print(results.richness)\n",
    "\n",
    "# In this example the second site showed a significant decrease in Richness over time (p<0.05)\n",
    "\n",
    "final_results <- merge(md,results.richness, by = \"siteid\")\n",
    "print(final_results)\n",
    "\n",
    "# Store results as CSV to temporary file\n",
    "results_filename <- \"biodiversity_trend_analysis_results.csv\"\n",
    "results_file_path <- paste(conf_temporary_data_directory, results_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing final_results to %s\", results_file_path))\n",
    "write.csv(final_results, file = results_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceaafbe-9f4c-4546-830d-7d2886f4ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output storer\n",
    "Sys.setenv(\"AWS_S3_ENDPOINT\" = conf_minio_endpoint,\n",
    "           \"AWS_DEFAULT_REGION\" = conf_minio_region,\n",
    "           \"AWS_ACCESS_KEY_ID\" = secret_minio_access_key,\n",
    "           \"AWS_SECRET_ACCESS_KEY\" = secret_minio_secret_key)\n",
    "\n",
    "upload_file_to_bucket <- function(filename) {\n",
    "    aws.s3::put_object(bucket=conf_minio_user_bucket, file=paste(conf_temporary_data_directory, filename, sep=\"/\"), object=paste(param_user_name, filename, sep=\"/\"))\n",
    "}\n",
    "\n",
    "# Upload files to bucket\n",
    "upload_file_to_bucket(results_filename)\n",
    "upload_file_to_bucket(cleaned_metadata_filename)\n",
    "upload_file_to_bucket(cleaned_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264361b-4c21-48b0-b2b4-6046ac812e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:biotisan-euromarec]",
   "language": "R",
   "name": "conda-env-biotisan-euromarec-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
