{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "477a0544-ba44-4649-87a2-4448aa7bafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (do not containerize this cell)\n",
    "param_data_filename <- \"Template_MBO_Example_raw.xlsx\"\n",
    "param_metadata_sheet <- \"METADATA\"\n",
    "param_data_sheet <- \"BIRDS\"\n",
    "param_user_name = \"See URL: beta.naavre.net/jupyter/user/[param_user_name]/lab\"\n",
    "param_use_dummy_data <- 1\n",
    "param_years <- 7\n",
    "param_latitude_north <- 90.0000\n",
    "param_latitude_south <- 25.0000\n",
    "param_longitude_east <- 70.0000\n",
    "param_longitude_west <- -45.0000\n",
    "param_upper_limit_max_depth <- 0\n",
    "param_lower_limit_max_depth <- 50\n",
    "param_upper_limit_min_depth <- 0\n",
    "param_lower_limit_min_depth <- 50\n",
    "param_first_month <- 1\n",
    "param_last_month <- 3\n",
    "conf_temporary_data_directory <- \"/tmp/data\"\n",
    "conf_virtual_lab_biotisan_euromarec <- \"vl-biotisan-euromarec\"\n",
    "conf_minio_endpoint <- \"scruffy.lab.uvalight.net:9000\"\n",
    "conf_minio_region <- \"nl-uvalight\"\n",
    "conf_minio_public_bucket <- \"naa-vre-public\"\n",
    "conf_minio_user_bucket <- \"naa-vre-user-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a07a2837-0613-49ac-9092-2cf4f73dac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secrets (do not containerize this cell)\n",
    "library(\"SecretsProvider\")\n",
    "\n",
    "secretsProvider <- SecretsProvider()\n",
    "secret_minio_access_key = \"\"\n",
    "secret_minio_access_key = secretsProvider$get_secret(\"secret_minio_access_key\")\n",
    "secret_minio_secret_key = \"\"\n",
    "secret_minio_secret_key = secretsProvider$get_secret(\"secret_minio_secret_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9a7a47a-31d5-4c61-9bae-183b1d07d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Using dummy data for testing purposes. Set param_use_dummy_data to 0 to use your own data. Downloading data from naa-vre-public/vl-biotisan-euromarec/Template_MBO_Example_raw.xlsx\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'Template_MBO_Example_raw.xlsx'"
      ],
      "text/latex": [
       "'Template\\_MBO\\_Example\\_raw.xlsx'"
      ],
      "text/markdown": [
       "'Template_MBO_Example_raw.xlsx'"
      ],
      "text/plain": [
       "[1] \"Template_MBO_Example_raw.xlsx\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Storing metadata in: /tmp/data/metadata.csv, and data in /tmp/data/data.csv\"\n"
     ]
    }
   ],
   "source": [
    "# MinIO data retriever\n",
    "library(\"readxl\")\n",
    "library(\"aws.s3\")\n",
    "\n",
    "Sys.setenv(\"AWS_S3_ENDPOINT\" = conf_minio_endpoint,\n",
    "           \"AWS_DEFAULT_REGION\" = conf_minio_region,\n",
    "           \"AWS_ACCESS_KEY_ID\" = secret_minio_access_key,\n",
    "           \"AWS_SECRET_ACCESS_KEY\" = secret_minio_secret_key)\n",
    "\n",
    "# Download file from bucket S3\n",
    "if (param_use_dummy_data) {\n",
    "        file_path <- paste(conf_virtual_lab_biotisan_euromarec, param_data_filename, sep=\"/\")\n",
    "        print(sprintf(\"Using dummy data for testing purposes. Set param_use_dummy_data to 0 to use your own data. Downloading data from %s/%s\", conf_minio_public_bucket, file_path))\n",
    "        aws.s3::save_object(bucket=conf_minio_public_bucket, object=file_path, file=param_data_filename)\n",
    "    } else {\n",
    "        file_path <- paste(param_user_name, param_data_filename, sep=\"/\")\n",
    "        print(sprintf(\"Downloading data from %s / %s\", conf_minio_user_bucket, file_path))\n",
    "        aws.s3::save_object(bucket=conf_minio_user_bucket, object=file_path, file=param_data_filename)\n",
    "}\n",
    "\n",
    "# Load data & metadata\n",
    "metadata <- read_excel(param_data_filename, sheet = param_metadata_sheet) #Load metadata sheet\n",
    "data <- read_excel(param_data_filename, sheet = param_data_sheet) #Load data sheet\n",
    "\n",
    "# Ensure the temporary data storage directory exists\n",
    "dir.create(conf_temporary_data_directory, showWarnings = FALSE)\n",
    "\n",
    "# Write (meta)data to files\n",
    "metadata_as_csv_filename <- \"metadata.csv\"\n",
    "data_as_csv_filename <- \"data.csv\"\n",
    "metadata_from_excel_path <- paste(conf_temporary_data_directory, metadata_as_csv_filename, sep=\"/\")\n",
    "data_from_excel_path <- paste(conf_temporary_data_directory, data_as_csv_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing metadata in: %s, and data in %s\", metadata_from_excel_path, data_from_excel_path))\n",
    "write.csv(metadata, file = metadata_from_excel_path)\n",
    "write.csv(data, file =  data_from_excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14c595e0-33ba-4aae-890f-ad44c397429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Filtering sites with 7 or more years of data.\"\n",
      "[1] \"Filtering for data within the coordinates 25.000000 south, 90.000000 north, 70.000000 east and -45.000000 west.\"\n",
      "[1] \"Filtering for data within upper_limit_max_depth 0, lower_limit_max_depth 50, upper_limit_min_depth 0, lower_limit_min_depth 50.\"\n",
      "[1] \"Filtering for data between the month 1 and month 3.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'data read from csv has 239 rows.'"
      ],
      "text/latex": [
       "'data read from csv has 239 rows.'"
      ],
      "text/markdown": [
       "'data read from csv has 239 rows.'"
      ],
      "text/plain": [
       "[1] \"data read from csv has 239 rows.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'metadata read from csv has 3 rows.'"
      ],
      "text/latex": [
       "'metadata read from csv has 3 rows.'"
      ],
      "text/markdown": [
       "'metadata read from csv has 3 rows.'"
      ],
      "text/plain": [
       "[1] \"metadata read from csv has 3 rows.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'metadata merged by siteid has 2 rows.'"
      ],
      "text/latex": [
       "'metadata merged by siteid has 2 rows.'"
      ],
      "text/markdown": [
       "'metadata merged by siteid has 2 rows.'"
      ],
      "text/plain": [
       "[1] \"metadata merged by siteid has 2 rows.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'data after first years filter has 239 rows.'"
      ],
      "text/latex": [
       "'data after first years filter has 239 rows.'"
      ],
      "text/markdown": [
       "'data after first years filter has 239 rows.'"
      ],
      "text/plain": [
       "[1] \"data after first years filter has 239 rows.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        siteid decimallatitude decimallongitude\n",
      "1 57.89356369_22.04256156_6026        57.89356         22.04256\n",
      "2 57.90163988_22.04041014_6026        57.90164         22.04041\n",
      "[1] \"Number of sites found within the specified geolocation: 2\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'data filtered by coordinates has 164 rows.'"
      ],
      "text/latex": [
       "'data filtered by coordinates has 164 rows.'"
      ],
      "text/markdown": [
       "'data filtered by coordinates has 164 rows.'"
      ],
      "text/plain": [
       "[1] \"data filtered by coordinates has 164 rows.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'data filtered by depth has 164 rows.'"
      ],
      "text/latex": [
       "'data filtered by depth has 164 rows.'"
      ],
      "text/markdown": [
       "'data filtered by depth has 164 rows.'"
      ],
      "text/plain": [
       "[1] \"data filtered by depth has 164 rows.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'data filtered by season has 163 rows.'"
      ],
      "text/latex": [
       "'data filtered by season has 163 rows.'"
      ],
      "text/markdown": [
       "'data filtered by season has 163 rows.'"
      ],
      "text/plain": [
       "[1] \"data filtered by season has 163 rows.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'data filtered by years has 163 rows.'"
      ],
      "text/latex": [
       "'data filtered by years has 163 rows.'"
      ],
      "text/markdown": [
       "'data filtered by years has 163 rows.'"
      ],
      "text/plain": [
       "[1] \"data filtered by years has 163 rows.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Storing metadata in: /tmp/data/metadata_Example.csv, and data in /tmp/data/data_Example.csv\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "FALSE"
      ],
      "text/latex": [
       "FALSE"
      ],
      "text/markdown": [
       "FALSE"
      ],
      "text/plain": [
       "[1] FALSE\n",
       "attr(,\"response\")\n",
       "Response [https://nl-uvalight.scruffy.lab.uvalight.net:9000/naa-vre-user-data/koen.greuell%40lifewatch.eu/data_after_cleaning.csv]\n",
       "  Date: 2025-12-02 15:45\n",
       "  Status: 200\n",
       "  Content-Type: text/plain; charset=utf-8\n",
       "<EMPTY BODY>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Species occurence data cleaner\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "\n",
    "validate_dataframe_has_data <- function(dataframe, dataframe_name) {\n",
    "    if (nrow(dataframe) == 0) {\n",
    "    stop(paste0(dataframe_name, \" has no rows (0 rows). Halting execution.\"))\n",
    "  } else {\n",
    "    sprintf(\"%s has %i rows.\", dataframe_name, nrow(dataframe))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Report on parameters set\n",
    "print(sprintf(\"Filtering sites with %i or more years of data.\", param_years))\n",
    "print(sprintf(\"Filtering for data within the coordinates %f south, %f north, %f east and %f west.\", param_latitude_south, param_latitude_north, param_longitude_east, param_longitude_west))\n",
    "print(sprintf(\"Filtering for data within upper_limit_max_depth %i, lower_limit_max_depth %i, upper_limit_min_depth %i, lower_limit_min_depth %i.\", param_upper_limit_max_depth, param_lower_limit_max_depth, param_upper_limit_min_depth, param_lower_limit_min_depth))\n",
    "print(sprintf(\"Filtering for data between the month %i and month %i.\", param_first_month, param_last_month))\n",
    "\n",
    "# Assign dummy variables to prevent false Input/Output detection by the NaaVRE cell analyzer\n",
    "datecollected = \"\"\n",
    "siteid = \"\"\n",
    "decimallatitude = \"\"\n",
    "decimallongitude = \"\"\n",
    "\n",
    "# Read (meta)data from files\n",
    "md <- read.csv(paste(conf_temporary_data_directory, metadata_as_csv_filename, sep=\"/\"), sep=\",\")\n",
    "data <- read.csv(paste(conf_temporary_data_directory, data_as_csv_filename, sep=\"/\"), sep=\",\")\n",
    "validate_dataframe_has_data(data, \"data read from csv\")\n",
    "validate_dataframe_has_data(md, \"metadata read from csv\")\n",
    "\n",
    "###### Years filter ######\n",
    "###### The \"number of sampled years\" could be changed by the user (default = 7) ######\n",
    "# Create a table with sites with more than 7 sampling years\n",
    "sites <- data %>% \n",
    "  group_by(siteid) %>%\n",
    "  summarise(nyear = n_distinct(substr(datecollected, 1, 4))) %>%\n",
    "  filter(nyear > param_years)\n",
    "\n",
    "md <- merge(md,sites, by = \"siteid\")\n",
    "validate_dataframe_has_data(md, \"metadata merged by siteid\")\n",
    "validate_dataframe_has_data(data, \"data after first years filter\")\n",
    "\n",
    "###### Coordinates filter ######\n",
    "###### The \"geographical boundaries\" could be changed by the user (default = latitude (25:90), longitude (-45:70)). The default values correspond to European continental waters. ######\n",
    "# Keep sites within the study area [our boundaries are latitude (25:90), longitude (-45:70)]\n",
    "metadata_coordinates <- md %>% select(siteid, decimallatitude, decimallongitude)\n",
    "print(metadata_coordinates)\n",
    "\n",
    "# md <- dplyr::filter(md, decimallatitude >= 57, decimallatitude <= 60, \n",
    "#                  decimallongitude >= 22, decimallongitude <= 23)\n",
    "md <- dplyr::filter(md, decimallatitude >= param_latitude_south, decimallatitude <= param_latitude_north, \n",
    "                 decimallongitude >= param_longitude_west, decimallongitude <= param_longitude_east)\n",
    "print(paste0(\"Number of sites found within the specified geolocation: \", nrow(md)))\n",
    "\n",
    "data <- data %>% # Keep data from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "validate_dataframe_has_data(data, \"data filtered by coordinates\")\n",
    "\n",
    "###### Depth filter ######\n",
    "###### The \"depth\" could be changed by the user (default = 0 to 50 meters). It should be given in absolute value. ######\n",
    "#(in this case is not necessary, but for other taxonomic groups is possible that the sample were taken at different depths. The code keeps the NAs in case that information is not known)\n",
    "data <- data %>% filter((maximumdepthinmeters >= param_upper_limit_max_depth & maximumdepthinmeters <= param_lower_limit_max_depth) %>% tidyr::replace_na(TRUE))\n",
    "data <- data %>% filter((minimumdepthinmeters >= param_upper_limit_min_depth & minimumdepthinmeters <= param_lower_limit_min_depth) %>% tidyr::replace_na(TRUE))\n",
    "validate_dataframe_has_data(data, \"data filtered by depth\")\n",
    "\n",
    "###### Season filter ######\n",
    "###### The \"season\" could be changed by the user (default = 1:3). The default values correspond to winter.\n",
    "# The period does not need to be three months, can be 1:12 for the whole year. It cannot choose months from different years (for example, November to January) ######\n",
    "# In this case, most of the sampling campaigns were conducted in winter\n",
    "# One was conducted in summer and should be removed since the sampling season is not consistent\n",
    "data$month <- as.numeric(format(as.Date(data$datecollected), \"%m\")) # Create a column with the sampling month\n",
    "\n",
    "season <- c(param_first_month:param_last_month)\n",
    "data <- data %>%\n",
    "  filter(month %in% season) #Remove those samples in non-consistent seasons (in this case keeps the months 1, 2 and 3, this is January, February and March)\n",
    "validate_dataframe_has_data(data, \"data filtered by season\")\n",
    "\n",
    "# Note that some time series can have more than one sampling campaign per year and even per season (not in this case)\n",
    "# For our analysis, we are only keeping one sampling campaign per year\n",
    "\n",
    "###### Years filter ######\n",
    "###### Note that this step is repeated ######\n",
    "# Update the table with sites with more than the number of sampled years\n",
    "# After removing inconsistent sampling campaigns, some time series may become shorter than 8 years\n",
    "sites <- data %>% \n",
    "  group_by(siteid) %>%\n",
    "  summarise(nyear = n_distinct(substr(datecollected, 1, 4))) %>%\n",
    "  filter(nyear > param_years)\n",
    "\n",
    "data <- data %>% # Keep data from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "md <- md %>% # Keep metadata from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "validate_dataframe_has_data(data, \"data filtered by years\")\n",
    "\n",
    "md_final <- md[,c(1:8)]\n",
    "data_final <- data[,c(1:15)]\n",
    "\n",
    "# Write data to files\n",
    "cleaned_metadata_filename <- \"metadata_Example.csv\"\n",
    "cleaned_data_filename <- \"data_Example.csv\"\n",
    "cleaned_metadata_path <- paste(conf_temporary_data_directory, cleaned_metadata_filename, sep=\"/\")\n",
    "cleaned_data_path <- paste(conf_temporary_data_directory, cleaned_data_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing metadata in: %s, and data in %s\", cleaned_metadata_path, cleaned_data_path))\n",
    "write.csv(md_final, file = cleaned_metadata_path)\n",
    "write.csv(data_final, file =  cleaned_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19e93b1c-73ab-44cd-9a54-4674d75a6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Retrieving metadata from: /tmp/data/metadata_Example.csv, and data from /tmp/data/data_Example.csv\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'X.1'</li><li>'X'</li><li>'datecollected'</li><li>'siteid'</li><li>'sampleid'</li><li>'basisofrecord'</li><li>'minimumdepthinmeters'</li><li>'maximumdepthinmeters'</li><li>'taxaname'</li><li>'taxanameid'</li><li>'samplingeffort'</li><li>'parameter'</li><li>'parameter_value'</li><li>'parameter_standardunit'</li><li>'wetweightgrams'</li><li>'dryweigthgrams'</li><li>'year'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'X.1'\n",
       "\\item 'X'\n",
       "\\item 'datecollected'\n",
       "\\item 'siteid'\n",
       "\\item 'sampleid'\n",
       "\\item 'basisofrecord'\n",
       "\\item 'minimumdepthinmeters'\n",
       "\\item 'maximumdepthinmeters'\n",
       "\\item 'taxaname'\n",
       "\\item 'taxanameid'\n",
       "\\item 'samplingeffort'\n",
       "\\item 'parameter'\n",
       "\\item 'parameter\\_value'\n",
       "\\item 'parameter\\_standardunit'\n",
       "\\item 'wetweightgrams'\n",
       "\\item 'dryweigthgrams'\n",
       "\\item 'year'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'X.1'\n",
       "2. 'X'\n",
       "3. 'datecollected'\n",
       "4. 'siteid'\n",
       "5. 'sampleid'\n",
       "6. 'basisofrecord'\n",
       "7. 'minimumdepthinmeters'\n",
       "8. 'maximumdepthinmeters'\n",
       "9. 'taxaname'\n",
       "10. 'taxanameid'\n",
       "11. 'samplingeffort'\n",
       "12. 'parameter'\n",
       "13. 'parameter_value'\n",
       "14. 'parameter_standardunit'\n",
       "15. 'wetweightgrams'\n",
       "16. 'dryweigthgrams'\n",
       "17. 'year'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"X.1\"                    \"X\"                      \"datecollected\"         \n",
       " [4] \"siteid\"                 \"sampleid\"               \"basisofrecord\"         \n",
       " [7] \"minimumdepthinmeters\"   \"maximumdepthinmeters\"   \"taxaname\"              \n",
       "[10] \"taxanameid\"             \"samplingeffort\"         \"parameter\"             \n",
       "[13] \"parameter_value\"        \"parameter_standardunit\" \"wetweightgrams\"        \n",
       "[16] \"dryweigthgrams\"         \"year\"                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'The data has 163 rows:'"
      ],
      "text/latex": [
       "'The data has 163 rows:'"
      ],
      "text/markdown": [
       "'The data has 163 rows:'"
      ],
      "text/plain": [
       "[1] \"The data has 163 rows:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'siteid', 'year'. You can override using\n",
      "the `.groups` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            siteid       slope          p\n",
      "year  57.89356369_22.04256156_6026  0.02444139 0.08614967\n",
      "year1 57.90163988_22.04041014_6026 -0.01267176 0.02748902\n",
      "                        siteid X.1 X taxagroup     sitename country    seaname\n",
      "1 57.89356369_22.04256156_6026   1 1     Birds EN_6026_0001      EE Baltic Sea\n",
      "2 57.90163988_22.04041014_6026   2 2     Birds EN_6026_0002      EE Baltic Sea\n",
      "  decimallatitude decimallongitude       slope          p\n",
      "1        57.89356         22.04256  0.02444139 0.08614967\n",
      "2        57.90164         22.04041 -0.01267176 0.02748902\n",
      "[1] \"Storing final_results to /tmp/data/biodiversity_trend_analysis_results.csv\"\n"
     ]
    }
   ],
   "source": [
    "# Trend analyzer\n",
    "library(vegan)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(nlme)\n",
    "\n",
    "# Assign dummy variables to prevent false Input/Output detection by the NaaVRE cell analyzer\n",
    "datecollected = \"\"\n",
    "parameter_value = \"\"\n",
    "\n",
    "# Load cleaned data & metadata\n",
    "cleaned_metadata_path <- paste(conf_temporary_data_directory, cleaned_metadata_filename, sep=\"/\")\n",
    "cleaned_data_path <- paste(conf_temporary_data_directory, cleaned_data_filename, sep=\"/\")\n",
    "print(sprintf(\"Retrieving metadata from: %s, and data from %s\", cleaned_metadata_path, cleaned_data_path))\n",
    "md <- read.csv(cleaned_metadata_path, sep=\",\")\n",
    "data <- read.csv(cleaned_data_path, sep=\",\")\n",
    "data$year <- as.numeric(format(as.Date(data$datecollected), \"%Y\"))\n",
    "colnames(data)\n",
    "\n",
    "# Validate data exists\n",
    "if (nrow(data) == 0) {\n",
    "    stop(\"data has no rows (0 rows). Halting execution.\")\n",
    "  } else {\n",
    "    sprintf(\"The data has %i rows:\", nrow(data))\n",
    "    }\n",
    "\n",
    "# Calculate community metrics\n",
    "data.tax <- data %>%\n",
    "  group_by(siteid, year, datecollected) %>%\n",
    "  summarise(richness = n_distinct(taxaname[parameter_value > 0]), # Richness\n",
    "            parameter_value_tot = sum(parameter_value), # Abundance estimate\n",
    "            parameter = unique(parameter),\n",
    "            parameter_standardunit = unique(parameter_standardunit),\n",
    "            diversity = diversity(parameter_value, index=\"shannon\"), # Diversity\n",
    "            )\n",
    "\n",
    "# Temporal analysis. Example with Richness and these 2 time series\n",
    "results.richness <- data.frame(siteid = character(0), slope = numeric(0), p = numeric(0))\n",
    "\n",
    "for (i in names(table(data.tax$siteid))) {\n",
    "  x <- subset(data.tax, siteid == i)\n",
    "  # We used GLS models taking into account the temporal autocorrelation\n",
    "  gls_model <- gls(log10(richness+1) ~ year, data = x, correlation = corAR1(form = ~ 1 | year))\n",
    "  slope <- coef(gls_model)[2]\n",
    "  p <- summary(gls_model)$tTable[2, 4]\n",
    "\n",
    "  # Save results\n",
    "  results.richness <- rbind(results.richness, data.frame(siteid = i, slope = slope, p = p))\n",
    "}\n",
    "\n",
    "print(results.richness)\n",
    "\n",
    "# In this example the second site showed a significant decrease in Richness over time (p<0.05)\n",
    "\n",
    "final_results <- merge(md,results.richness, by = \"siteid\")\n",
    "print(final_results)\n",
    "\n",
    "# Store results as CSV to temporary file\n",
    "results_filename <- \"biodiversity_trend_analysis_results.csv\"\n",
    "results_file_path <- paste(conf_temporary_data_directory, results_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing final_results to %s\", results_file_path))\n",
    "write.csv(final_results, file = results_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ceaafbe-9f4c-4546-830d-7d2886f4ca47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "FALSE"
      ],
      "text/latex": [
       "FALSE"
      ],
      "text/markdown": [
       "FALSE"
      ],
      "text/plain": [
       "[1] FALSE\n",
       "attr(,\"response\")\n",
       "Response [https://nl-uvalight.scruffy.lab.uvalight.net:9000/naa-vre-user-data/koen.greuell%40lifewatch.eu/biodiversity_trend_analysis_results.csv]\n",
       "  Date: 2025-12-02 12:32\n",
       "  Status: 200\n",
       "  Content-Type: text/plain; charset=utf-8\n",
       "<EMPTY BODY>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output storer\n",
    "Sys.setenv(\"AWS_S3_ENDPOINT\" = conf_minio_endpoint,\n",
    "           \"AWS_DEFAULT_REGION\" = conf_minio_region,\n",
    "           \"AWS_ACCESS_KEY_ID\" = secret_minio_access_key,\n",
    "           \"AWS_SECRET_ACCESS_KEY\" = secret_minio_secret_key)\n",
    "\n",
    "upload_file_to_bucket <- function(filename) {\n",
    "    aws.s3::put_object(bucket=conf_minio_user_bucket, file=paste(conf_temporary_data_directory, filename, sep=\"/\"), object=paste(param_user_name, filename, sep=\"/\"))\n",
    "}\n",
    "\n",
    "# Upload files to bucket\n",
    "upload_file_to_bucket(results_filename)\n",
    "upload_file_to_bucket(cleaned_metadata_filename)\n",
    "upload_file_to_bucket(cleaned_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264361b-4c21-48b0-b2b4-6046ac812e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:biotisan-euromarec]",
   "language": "R",
   "name": "conda-env-biotisan-euromarec-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
