{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a0544-ba44-4649-87a2-4448aa7bafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (do not containerize this cell)\n",
    "param_data_filename <- \"Template_MBO_Example_raw.xlsx\"\n",
    "param_metadata_sheet <- \"METADATA\"\n",
    "param_data_sheet <- \"BIRDS\"\n",
    "param_user_name <- \"koen.greuell@lifewatch.eu\" #\"See URL: beta.naavre.net/jupyter/user/[param_user_name]/lab\"\n",
    "param_use_dummy_data <- 1\n",
    "param_years <- 7\n",
    "param_latitude_north <- 90.0000\n",
    "param_latitude_south <- 25.0000\n",
    "param_longitude_east <- 70.0000\n",
    "param_longitude_west <- 00.0000\n",
    "param_upper_limit_max_depth <- 0\n",
    "param_lower_limit_max_depth <- 50\n",
    "param_upper_limit_min_depth <- 0\n",
    "param_lower_limit_min_depth <- 50\n",
    "param_first_month <- 1\n",
    "param_last_month <- 3\n",
    "param_output_samples_ecological_parameters <- 0\n",
    "param_make_plots <- 1\n",
    "param_transform_to_log10 <- 1\n",
    "conf_temporary_data_directory <- \"/tmp/data\"\n",
    "conf_virtual_lab_biotisan_euromarec <- \"vl-biotisan-euromarec\"\n",
    "conf_minio_endpoint <- \"scruffy.lab.uvalight.net:9000\"\n",
    "conf_minio_region <- \"nl-uvalight\"\n",
    "conf_minio_public_bucket <- \"naa-vre-public\"\n",
    "conf_minio_user_bucket <- \"naa-vre-user-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a2837-0613-49ac-9092-2cf4f73dac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secrets (do not containerize this cell)\n",
    "library(\"SecretsProvider\")\n",
    "\n",
    "secretsProvider <- SecretsProvider()\n",
    "secret_minio_access_key = \"\"\n",
    "secret_minio_access_key = secretsProvider$get_secret(\"secret_minio_access_key\")\n",
    "secret_minio_secret_key = \"\"\n",
    "secret_minio_secret_key = secretsProvider$get_secret(\"secret_minio_secret_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7a47a-31d5-4c61-9bae-183b1d07d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinIO data retriever\n",
    "library(\"readxl\")\n",
    "library(\"aws.s3\")\n",
    "\n",
    "Sys.setenv(\"AWS_S3_ENDPOINT\" = conf_minio_endpoint,\n",
    "           \"AWS_DEFAULT_REGION\" = conf_minio_region,\n",
    "           \"AWS_ACCESS_KEY_ID\" = secret_minio_access_key,\n",
    "           \"AWS_SECRET_ACCESS_KEY\" = secret_minio_secret_key)\n",
    "\n",
    "# Ensure the temporary data storage directory exists\n",
    "dir.create(conf_temporary_data_directory, showWarnings = FALSE)\n",
    "\n",
    "# Download file from bucket S3\n",
    "if (param_use_dummy_data) {\n",
    "        file_path <- paste(conf_virtual_lab_biotisan_euromarec, param_data_filename, sep=\"/\")\n",
    "        print(sprintf(\"Using dummy data for testing purposes. Set param_use_dummy_data to 0 to use your own data. Downloading data from bucket: %s folder: %s\", conf_minio_public_bucket, file_path))\n",
    "        aws.s3::save_object(bucket=conf_minio_public_bucket, object=file_path, file=paste(conf_temporary_data_directory, param_data_filename, sep=\"/\"))\n",
    "    } else {\n",
    "        file_path <- paste(param_user_name, param_data_filename, sep=\"/\")\n",
    "        print(sprintf(\"Downloading data from bucket: %s folder: %s\", conf_minio_user_bucket, file_path))\n",
    "        aws.s3::save_object(bucket=conf_minio_user_bucket, object=file_path, file=paste(conf_temporary_data_directory, param_data_filename, sep=\"/\"))\n",
    "}\n",
    "\n",
    "# Load data & metadata\n",
    "metadata <- read_excel(paste(conf_temporary_data_directory, param_data_filename, sep=\"/\"), sheet = param_metadata_sheet) #Load metadata sheet\n",
    "data <- read_excel(paste(conf_temporary_data_directory, param_data_filename, sep=\"/\"), sheet = param_data_sheet) #Load data sheet\n",
    "\n",
    "# Validate column names\n",
    "validate_column_names <- function(required_column_names, dataframe) {\n",
    "    if (!all(required_column_names %in% colnames(dataframe))) {\n",
    "    missing_columns <- required_column_names[!required_column_names %in% colnames(dataframe)]\n",
    "    not_required_columns <- colnames(dataframe)[!colnames(dataframe) %in% required_column_names]\n",
    "    stop(paste(\"\\n\", deparse(substitute(dataframe)), \"does not contain all required column names. \\n The following columns are missing: \", paste(missing_columns, collapse=\", \"),\n",
    "              \"\\n The following column names are present but not required: \", paste(not_required_columns, collapse=\", \")))\n",
    "    } else {\n",
    "    sprintf(\"%s has the required column names.\", param_data_filename)\n",
    "    }\n",
    "}\n",
    "\n",
    "required_data_columns <- c(\n",
    "    'datecollected',\n",
    "    'siteid',\n",
    "    'sampleid',\n",
    "    'basisofrecord',\n",
    "    'minimumdepthinmeters',\n",
    "    'maximumdepthinmeters',\n",
    "    'taxaname',\n",
    "    'taxanameid',\n",
    "    'samplingeffort',\n",
    "    'parameter',\n",
    "    'parameter_value',\n",
    "    'parameter_standardunit',\n",
    "    'wetweightgrams',\n",
    "    'dryweigthgrams',\n",
    "    'afdw'\n",
    ")\n",
    "required_metadata_columns <- c(\n",
    "    'siteid',\n",
    "    'decimallatitude', \n",
    "    'decimallongitude'\n",
    ")\n",
    "# validate_column_names(required_data_columns, data)\n",
    "validate_column_names(required_metadata_columns, metadata)\n",
    "\n",
    "# Write (meta)data to files\n",
    "metadata_as_csv_filename <- \"metadata.csv\"\n",
    "data_as_csv_filename <- \"data.csv\"\n",
    "metadata_from_excel_path <- paste(conf_temporary_data_directory, metadata_as_csv_filename, sep=\"/\")\n",
    "data_from_excel_path <- paste(conf_temporary_data_directory, data_as_csv_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing metadata in: %s, and data in %s\", metadata_from_excel_path, data_from_excel_path))\n",
    "write.csv(metadata, file = metadata_from_excel_path)\n",
    "write.csv(data, file =  data_from_excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030c326-67b4-43d9-9213-8d58857b5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter validator\n",
    "Sys.setenv(\"AWS_S3_ENDPOINT\" = conf_minio_endpoint,\n",
    "           \"AWS_DEFAULT_REGION\" = conf_minio_region,\n",
    "           \"AWS_ACCESS_KEY_ID\" = secret_minio_access_key,\n",
    "           \"AWS_SECRET_ACCESS_KEY\" = secret_minio_secret_key)\n",
    "\n",
    "upload_file_to_bucket <- function(filename) {\n",
    "    aws.s3::put_object(bucket=conf_minio_user_bucket, file=filename, object=paste(param_user_name, filename, sep=\"/\"))\n",
    "}\n",
    "\n",
    "number_of_validation_errors <- 0\n",
    "current_datetime <- gsub(\"\\\\s\", \"_\", format(Sys.time(), \"%Y%m%dT%H%M%SZ\"))\n",
    "validation_log_file <- paste(current_datetime, \"validation_log.txt\", sep=\"_\")\n",
    "\n",
    "report_validation_error <- function(validation_failure_message) {\n",
    "    number_of_validation_errors <<- number_of_validation_errors + 1\n",
    "    cat(validation_failure_message, file = validation_log_file, append = TRUE)\n",
    "    message(sprintf(validation_failure_message))\n",
    "    return(number_of_validation_errors)\n",
    "}\n",
    "\n",
    "validate_is_number_between <- function(number, minimum, maximum) {\n",
    "   if (!(is.numeric(number) & (number >= minimum) & (number <= maximum))) {\n",
    "       report_validation_error(sprintf(\"\\n %s (value: %s) is not: numeric and between %i and %i.\", deparse(substitute(number)), number, minimum, maximum))\n",
    "    } else {\n",
    "        message(sprintf(\"%s = %i passed validation: It is an integer between %i and %i\", deparse(substitute(number)), number, minimum, maximum))\n",
    "    } \n",
    "}\n",
    "\n",
    "validate_is_coordinate <- function(coordinate, pattern) {\n",
    "    is_valid_format <- grepl(pattern, coordinate)\n",
    "    if (!is_valid_format) {\n",
    "      report_validation_error(paste0(\"The coordinate: \", coordinate, \" does not follow the required \", deparse(substitute(pattern)), \": \", pattern, \" e.g. -56.0953\"))\n",
    "    } else {\n",
    "      message(sprintf(\"%s = %s passed validation: It is valid geographic coordinate input.\", deparse(substitute(coordinate)), coordinate))\n",
    "    }\n",
    "}\n",
    "\n",
    "validate_is_number_between(param_years, 0, 1000)\n",
    "\n",
    "latitude_pattern <- \"^-?((90(\\\\.0+)?)|([0-8]?\\\\d)(\\\\.\\\\d+)?)$\"\n",
    "longitude_pattern <- \"^-?((1[0-7]\\\\d(\\\\.\\\\d+)?)|(180(\\\\.0+)?)|(\\\\d{1,2}(\\\\.\\\\d+)?))$\"\n",
    "validate_is_coordinate(param_latitude_south, latitude_pattern)\n",
    "validate_is_coordinate(param_latitude_north, latitude_pattern)\n",
    "validate_is_coordinate(param_longitude_east, longitude_pattern)\n",
    "validate_is_coordinate(param_longitude_west, longitude_pattern)\n",
    "\n",
    "validate_is_number_between(param_upper_limit_max_depth, -1000, 1000)\n",
    "validate_is_number_between(param_lower_limit_max_depth, -1000, 1000)\n",
    "validate_is_number_between(param_upper_limit_min_depth, -1000, 1000)\n",
    "validate_is_number_between(param_lower_limit_min_depth, -1000, 1000)\n",
    "\n",
    "validate_is_number_between(param_first_month, 1, 12)\n",
    "validate_is_number_between(param_last_month, 1, 12)\n",
    "\n",
    "if (number_of_validation_errors > 0) {\n",
    "    upload_file_to_bucket(validation_log_file)\n",
    "    stop(sprintf(\"Parameter validation failed. See %s for errors\", validation_log_file))\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c595e0-33ba-4aae-890f-ad44c397429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Species occurence data cleaner\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "\n",
    "print(paste(deparse(substitute(number_of_validation_errors)), number_of_validation_errors, sep=\" = \"))\n",
    "\n",
    "validate_dataframe_has_data <- function(dataframe, dataframe_name) {\n",
    "    if (nrow(dataframe) == 0) {\n",
    "    stop(paste0(dataframe_name, \" has no rows (0 rows). Halting execution.\"))\n",
    "  } else {\n",
    "    sprintf(\"%s has %i rows.\", dataframe_name, nrow(dataframe))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Report on parameters set\n",
    "print(sprintf(\"Filtering sites with %i or more years of data.\", param_years))\n",
    "print(sprintf(\"Filtering for data within the coordinates %s south, %s north, %s east and %s west.\", param_latitude_south, param_latitude_north, param_longitude_east, param_longitude_west))\n",
    "print(sprintf(\"Filtering for data within upper_limit_max_depth %i, lower_limit_max_depth %i, upper_limit_min_depth %i, lower_limit_min_depth %i.\", param_upper_limit_max_depth, param_lower_limit_max_depth, param_upper_limit_min_depth, param_lower_limit_min_depth))\n",
    "print(sprintf(\"Filtering for data between the month %i and month %i.\", param_first_month, param_last_month))\n",
    "\n",
    "# Assign dummy variables to prevent false Input/Output detection by the NaaVRE cell analyzer\n",
    "datecollected = \"\"\n",
    "siteid = \"\"\n",
    "decimallatitude = \"\"\n",
    "decimallongitude = \"\"\n",
    "\n",
    "# Read (meta)data from files\n",
    "md <- read.csv(paste(conf_temporary_data_directory, metadata_as_csv_filename, sep=\"/\"), sep=\",\")\n",
    "data <- read.csv(paste(conf_temporary_data_directory, data_as_csv_filename, sep=\"/\"), sep=\",\")\n",
    "validate_dataframe_has_data(data, \"data read from csv\")\n",
    "validate_dataframe_has_data(md, \"metadata read from csv\")\n",
    "\n",
    "###### Years filter ######\n",
    "###### The \"number of sampled years\" could be changed by the user (default = 7) ######\n",
    "# Create a table with sites with more than 7 sampling years\n",
    "sites <- data %>% \n",
    "  group_by(siteid) %>%\n",
    "  summarise(nyear = n_distinct(substr(datecollected, 1, 4))) %>%\n",
    "  filter(nyear > param_years)\n",
    "\n",
    "md <- merge(md,sites, by = \"siteid\")\n",
    "validate_dataframe_has_data(md, \"metadata merged by siteid\")\n",
    "validate_dataframe_has_data(data, \"data after first years filter\")\n",
    "\n",
    "###### Coordinates filter ######\n",
    "###### The \"geographical boundaries\" could be changed by the user (default = latitude (25:90), longitude (-45:70)). The default values correspond to European continental waters. ######\n",
    "# Keep sites within the study area [our boundaries are latitude (25:90), longitude (-45:70)]\n",
    "metadata_coordinates <- md %>% select(siteid, decimallatitude, decimallongitude)\n",
    "print(metadata_coordinates)\n",
    "\n",
    "md <- dplyr::filter(md, decimallatitude >= param_latitude_south, decimallatitude <= param_latitude_north, \n",
    "                 decimallongitude >= param_longitude_west, decimallongitude <= param_longitude_east)\n",
    "print(paste0(\"Number of sites found within the specified geolocation: \", nrow(md)))\n",
    "\n",
    "data <- data %>% # Keep data from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "validate_dataframe_has_data(data, \"data filtered by coordinates\")\n",
    "\n",
    "###### Depth filter ######\n",
    "###### The \"depth\" could be changed by the user (default = 0 to 50 meters). It should be given in absolute value. ######\n",
    "#(in this case is not necessary, but for other taxonomic groups is possible that the sample were taken at different depths. The code keeps the NAs in case that information is not known)\n",
    "data <- data %>% filter((maximumdepthinmeters >= param_upper_limit_max_depth & maximumdepthinmeters <= param_lower_limit_max_depth) %>% tidyr::replace_na(TRUE))\n",
    "data <- data %>% filter((minimumdepthinmeters >= param_upper_limit_min_depth & minimumdepthinmeters <= param_lower_limit_min_depth) %>% tidyr::replace_na(TRUE))\n",
    "validate_dataframe_has_data(data, \"data filtered by depth\")\n",
    "\n",
    "###### Season filter ######\n",
    "###### The \"season\" could be changed by the user (default = 1:3). The default values correspond to winter.\n",
    "# The period does not need to be three months, can be 1:12 for the whole year. It cannot choose months from different years (for example, November to January) ######\n",
    "# In this case, most of the sampling campaigns were conducted in winter\n",
    "# One was conducted in summer and should be removed since the sampling season is not consistent\n",
    "data$month <- as.numeric(format(as.Date(data$datecollected), \"%m\")) # Create a column with the sampling month\n",
    "\n",
    "season <- c(param_first_month:param_last_month)\n",
    "data <- data %>%\n",
    "  filter(month %in% season) #Remove those samples in non-consistent seasons (in this case keeps the months 1, 2 and 3, this is January, February and March)\n",
    "validate_dataframe_has_data(data, \"data filtered by season\")\n",
    "\n",
    "# Note that some time series can have more than one sampling campaign per year and even per season (not in this case)\n",
    "# For our analysis, we are only keeping one sampling campaign per year\n",
    "\n",
    "###### Years filter ######\n",
    "###### Note that this step is repeated ######\n",
    "# Update the table with sites with more than the number of sampled years\n",
    "# After removing inconsistent sampling campaigns, some time series may become shorter than 8 years\n",
    "sites <- data %>% \n",
    "  group_by(siteid) %>%\n",
    "  summarise(nyear = n_distinct(substr(datecollected, 1, 4))) %>%\n",
    "  filter(nyear > param_years)\n",
    "\n",
    "data <- data %>% # Keep data from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "md <- md %>% # Keep metadata from these sites\n",
    "  filter(siteid %in% md$siteid)\n",
    "validate_dataframe_has_data(data, \"data filtered by years\")\n",
    "\n",
    "md_final <- md[,c(1:8)]\n",
    "data_final <- data[,c(1:15)]\n",
    "\n",
    "# Write data to files\n",
    "cleaned_metadata_filename <- \"metadata_Example.csv\"\n",
    "cleaned_data_filename <- \"data_Example.csv\"\n",
    "cleaned_metadata_path <- paste(conf_temporary_data_directory, cleaned_metadata_filename, sep=\"/\")\n",
    "cleaned_data_path <- paste(conf_temporary_data_directory, cleaned_data_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing metadata in: %s, and data in %s\", cleaned_metadata_path, cleaned_data_path))\n",
    "write.csv(md_final, file = cleaned_metadata_path)\n",
    "write.csv(data_final, file =  cleaned_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e93b1c-73ab-44cd-9a54-4674d75a6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend analyzer\n",
    "library(vegan)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(nlme)\n",
    "library(permute)\n",
    "\n",
    "# Assign dummy variables to prevent false Input/Output detection by the NaaVRE cell analyzer\n",
    "datecollected = \"\"\n",
    "parameter_value = \"\"\n",
    "\n",
    "# Load cleaned data & metadata\n",
    "cleaned_metadata_path <- paste(conf_temporary_data_directory, cleaned_metadata_filename, sep=\"/\")\n",
    "cleaned_data_path <- paste(conf_temporary_data_directory, cleaned_data_filename, sep=\"/\")\n",
    "print(sprintf(\"Retrieving metadata from: %s, and data from %s\", cleaned_metadata_path, cleaned_data_path))\n",
    "md <- read.csv(cleaned_metadata_path, sep=\",\")\n",
    "data <- read.csv(cleaned_data_path, sep=\",\")\n",
    "data$year <- as.numeric(format(as.Date(data$datecollected), \"%Y\"))\n",
    "colnames(data)\n",
    "\n",
    "# Calculate community metrics\n",
    "data.tax <- data %>%\n",
    "  group_by(siteid, year, datecollected) %>%\n",
    "  summarise(richness = n_distinct(taxaname[parameter_value > 0]), # Richness\n",
    "            abundance = sum(parameter_value), # Abundance estimate\n",
    "            parameter = unique(parameter),\n",
    "            parameter_standardunit = unique(parameter_standardunit),\n",
    "            diversity = diversity(parameter_value, index=\"shannon\"), # Diversity\n",
    "            )\n",
    "\n",
    "samples_ecological_parameters <- data.tax\n",
    "\n",
    "if (param_transform_to_log10) {\n",
    "    data.tax$richness <- log10(data.tax$richness+1)\n",
    "    data.tax$diversity <- log10(data.tax$diversity+1)\n",
    "    data.tax$abundance <- log10(data.tax$abundance+1)\n",
    "    }\n",
    "    \n",
    "# Depending on the analyses wanted, all the code can be run (for the three parameters) or just parts of it.\n",
    "# Temporal analysis. Example with Richness and these 2 time series\n",
    "results.richness <- data.frame(siteid = character(0), slope = numeric(0), p = numeric(0))\n",
    "\n",
    "for (i in names(table(data.tax$siteid))) {\n",
    "  x <- subset(data.tax, siteid == i)\n",
    "  # We used GLS models taking into account the temporal autocorrelation\n",
    "  gls_model <- gls(richness ~ year, data = x, correlation = corAR1(form = ~ 1 | year))\n",
    "  slope <- coef(gls_model)[2]  \n",
    "  p <- summary(gls_model)$tTable[2, 4]  \n",
    "  se <- summary(gls_model)$tTable[, \"Std.Error\"][2]\n",
    "  # Save results\n",
    "  results.richness <- rbind(results.richness, data.frame(siteid = i, slope_richness = slope, p_richness = p, se_richness = se))\n",
    "}\n",
    "\n",
    "results.richness$trend_richness <- if_else(results.richness$slope_richness > 0, if_else(results.richness$p_richness <= 0.05, \"Positive\", \"No change\"), \n",
    "                                  if_else(results.richness$p_richness <= 0.05, \"Negative\", \"No change\"))\n",
    "\n",
    "# Final results (just Richness)\n",
    "\n",
    "final_results_richness <- merge(md,results.richness, by.x = \"siteid\")\n",
    "\n",
    "\n",
    "# Temporal analysis. Example with Diversity and these 2 time series\n",
    "results.diversity <- data.frame(siteid = character(0), slope = numeric(0), p = numeric(0))\n",
    "\n",
    "for (i in names(table(data.tax$siteid))) {\n",
    "  x <- subset(data.tax, siteid == i)\n",
    "  # We used GLS models taking into account the temporal autocorrelation\n",
    "  gls_model <- gls(diversity ~ year, data = x, correlation = corAR1(form = ~ 1 | year))\n",
    "  slope <- coef(gls_model)[2]  \n",
    "  p <- summary(gls_model)$tTable[2, 4]  \n",
    "  se <- summary(gls_model)$tTable[, \"Std.Error\"][2]\n",
    "  # Save results\n",
    "  results.diversity <- rbind(results.diversity, data.frame(siteid = i, slope_diversity = slope, p_diversity = p, se_diversity = se))\n",
    "}\n",
    "\n",
    "results.diversity$trend_diversity <- if_else(results.diversity$slope_diversity > 0, if_else(results.diversity$p_diversity <= 0.05, \"Positive\", \"No change\"), \n",
    "                                           if_else(results.diversity$p_diversity <= 0.05, \"Negative\", \"No change\"))\n",
    "\n",
    "# Final results (just Diversity)\n",
    "\n",
    "final_results_diversity <- merge(md,results.diversity, by.x = \"siteid\")\n",
    "\n",
    "\n",
    "# Temporal analysis. Example with Abundance and these 2 time series\n",
    "results.abundance <- data.frame(siteid = character(0), slope = numeric(0), p = numeric(0))\n",
    "\n",
    "for (i in names(table(data.tax$siteid))) {\n",
    "  x <- subset(data.tax, siteid == i)\n",
    "  # We used GLS models taking into account the temporal autocorrelation\n",
    "  gls_model <- gls(abundance ~ year, data = x, correlation = corAR1(form = ~ 1 | year))\n",
    "  slope <- coef(gls_model)[2]  \n",
    "  p <- summary(gls_model)$tTable[2, 4]  \n",
    "  se <- summary(gls_model)$tTable[, \"Std.Error\"][2]\n",
    "  # Save results\n",
    "  results.abundance <- rbind(results.abundance, data.frame(siteid = i, slope_abundance = slope, p_abundance = p, se_abundance = se))\n",
    "}\n",
    "\n",
    "results.abundance$trend_abundance <- if_else(results.abundance$slope_abundance > 0, if_else(results.abundance$p_abundance <= 0.05, \"Positive\", \"No change\"), \n",
    "                                           if_else(results.abundance$p_abundance <= 0.05, \"Negative\", \"No change\"))\n",
    "\n",
    "# Final results (just Abundance)\n",
    "\n",
    "final_results_abundance <- merge(md,results.abundance, by.x = \"siteid\")\n",
    "\n",
    "\n",
    "# Final results (Richness & Diversity)\n",
    "\n",
    "final_results_richness_diversity <- merge(final_results_richness,results.diversity, by.x = \"siteid\")\n",
    "\n",
    "# Final results (Richness & Abundance)\n",
    "\n",
    "final_results_richness_abundance <- merge(final_results_richness,results.abundance, by.x = \"siteid\")\n",
    "\n",
    "# Final results (Diversity & Abundance)\n",
    "\n",
    "final_results_diversity_abundance <- merge(final_results_diversity,results.abundance, by.x = \"siteid\")\n",
    "\n",
    "\n",
    "# Final results (All metrics)\n",
    "\n",
    "final_results_all <- merge(final_results_richness_diversity,results.abundance, by.x = \"siteid\")\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "# Plots\n",
    "###### These plots show the frequency distribution of the time series along a \"percentage of change\" axis. ######\n",
    "# Zero means that the ecological metric did not change, -100 means that it decreased from its initial level to zero and +100 that it doubled from its initial level\n",
    "# The more sites included in the analysis, the more sense makes to plot the frequency distribution\n",
    "# There is the possibility of plotting all the variables together and of changing the axis size to better observe the results, but this is heavily case dependent\n",
    "\n",
    "# Richness\n",
    "results.richness$percent_change <- (10^(results.richness$slope) - 1) * 100\n",
    "\n",
    "plot.richness <- ggplot(data = results.richness, aes(x = percent_change)) +\n",
    "  geom_density(alpha = 0.5, color = \"#8a651b\", linewidth = 1.5) +\n",
    "  geom_vline(aes(xintercept = 0), color = \"black\", linetype = \"dashed\") +\n",
    "  theme_classic() +\n",
    "  labs(title = 'Frequency distribution - Richness',\n",
    "       x = \"Relative change (%)\",\n",
    "       y = \"\") +\n",
    "  theme(legend.title = element_blank(),\n",
    "        axis.text=element_text(size=10),\n",
    "        axis.line.y = element_blank(),\n",
    "        axis.text.y = element_blank(),\n",
    "        axis.ticks.y = element_blank(),\n",
    "        axis.title.y = element_blank(),\n",
    "        plot.title = element_text(size=16))+\n",
    "  xlim(c(-100,100))\n",
    "\n",
    "# Diversity\n",
    "results.diversity$percent_change <- (10^(results.diversity$slope) - 1) * 100\n",
    "\n",
    "plot.diversity <- ggplot(data = results.diversity, aes(x = percent_change)) +\n",
    "  geom_density(alpha = 0.5, color = \"#58326d\", linewidth = 1.5) +\n",
    "  geom_vline(aes(xintercept = 0), color = \"black\", linetype = \"dashed\") +\n",
    "  theme_classic() +\n",
    "  labs(title = 'Frequency distribution - Diversity',\n",
    "       x = \"Relative change (%)\",\n",
    "       y = \"\") +\n",
    "  theme(legend.title = element_blank(),\n",
    "        axis.text=element_text(size=10),\n",
    "        axis.line.y = element_blank(),\n",
    "        axis.text.y = element_blank(),\n",
    "        axis.ticks.y = element_blank(),\n",
    "        axis.title.y = element_blank(),\n",
    "        plot.title = element_text(size=16))+\n",
    "  xlim(c(-25,25))\n",
    "\n",
    "# Abundance\n",
    "results.abundance$percent_change <- (10^(results.abundance$slope) - 1) * 100\n",
    "\n",
    "plot.abundance <- ggplot(data = results.abundance, aes(x = percent_change)) +\n",
    "  geom_density(alpha = 0.5, color = \"#338888\", linewidth = 1.5) +\n",
    "  geom_vline(aes(xintercept = 0), color = \"black\", linetype = \"dashed\") +\n",
    "  theme_classic() +\n",
    "  labs(title = 'Frequency distribution - Abundance',\n",
    "       x = \"Relative change (%)\",\n",
    "       y = \"\") +\n",
    "  theme(legend.title = element_blank(),\n",
    "        axis.text=element_text(size=10),\n",
    "        axis.line.y = element_blank(),\n",
    "        axis.text.y = element_blank(),\n",
    "        axis.ticks.y = element_blank(),\n",
    "        axis.title.y = element_blank(),\n",
    "        plot.title = element_text(size=16))+\n",
    "  xlim(c(-100,100))\n",
    "\n",
    "######################################################################################################\n",
    "# Storing temporary files\n",
    "current_datetime <- gsub(\"\\\\s\", \"_\", format(Sys.time(), \"%Y%m%dT%H%M%SZ\"))\n",
    "data_filename_without_extension <- sub('\\\\.[^.]+$', '', param_data_filename)\n",
    "\n",
    "# Store results as CSV to temporary file\n",
    "output_dataframe_name <- deparse(substitute(final_results_all))\n",
    "results_filename <- \"\"\n",
    "results_filename <- paste0(paste(current_datetime, data_filename_without_extension, output_dataframe_name, sep=\"__\"), \".csv\") \n",
    "results_file_path <- paste(conf_temporary_data_directory, results_filename, sep=\"/\")\n",
    "print(sprintf(\"Storing final_results_all to %s\", results_file_path))\n",
    "write.csv(final_results_all, file = results_file_path)\n",
    "\n",
    "# Store samples_ecological_parameters as CSV to temporary file\n",
    "output_dataframe_name <- deparse(substitute(samples_ecological_parameters))\n",
    "samples_ecological_parameters_filename <- \"\"\n",
    "samples_ecological_parameters_filename <- paste0(paste(current_datetime, data_filename_without_extension, output_dataframe_name, sep=\"__\"), \".csv\") \n",
    "samples_file_path <- paste(conf_temporary_data_directory, samples_ecological_parameters_filename, sep=\"/\")\n",
    "write.csv(samples_ecological_parameters, file = samples_file_path)\n",
    "\n",
    "# Store the images\n",
    "save_plot_as_png <- function(graph) {\n",
    "    graph_name <- gsub('\\\\.','_',deparse(substitute(graph)))\n",
    "    graph_filename <- paste0(paste(current_datetime, data_filename_without_extension, graph_name, sep=\"__\"), \".png\")\n",
    "    graph_file_path <- paste(conf_temporary_data_directory, graph_filename, sep=\"/\")\n",
    "    ggsave(graph, filename = graph_file_path, device = \"png\", height = 8, width = 12, units = \"cm\")\n",
    "    return(graph_filename)\n",
    "    }\n",
    "plot_richness_filename <- \"\"\n",
    "plot_richness_filename <- save_plot_as_png(plot.richness)\n",
    "plot_diversity_filename <- \"\"\n",
    "plot_diversity_filename <- save_plot_as_png(plot.diversity)\n",
    "plot_abundance_filename <- \"\"\n",
    "plot_abundance_filename <- save_plot_as_png(plot.abundance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceaafbe-9f4c-4546-830d-7d2886f4ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output storer\n",
    "Sys.setenv(\"AWS_S3_ENDPOINT\" = conf_minio_endpoint,\n",
    "           \"AWS_DEFAULT_REGION\" = conf_minio_region,\n",
    "           \"AWS_ACCESS_KEY_ID\" = secret_minio_access_key,\n",
    "           \"AWS_SECRET_ACCESS_KEY\" = secret_minio_secret_key)\n",
    "\n",
    "# Upload files to bucket\n",
    "upload_file_to_bucket <- function(filename) {\n",
    "    aws.s3::put_object(bucket=conf_minio_user_bucket, file=paste(conf_temporary_data_directory, filename, sep=\"/\"), object=paste(param_user_name, filename, sep=\"/\"))\n",
    "}\n",
    "\n",
    "upload_file_to_bucket(results_filename)\n",
    "if (param_output_samples_ecological_parameters) {\n",
    "    upload_file_to_bucket(samples_ecological_parameters_filename)\n",
    "}\n",
    "if (param_make_plots) {\n",
    "    upload_file_to_bucket(plot_richness_filename)\n",
    "    upload_file_to_bucket(plot_diversity_filename)\n",
    "    upload_file_to_bucket(plot_abundance_filename)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c83712-351d-48de-88df-509c1f7e1c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:biotisan-euromarec]",
   "language": "R",
   "name": "conda-env-biotisan-euromarec-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
